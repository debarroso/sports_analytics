# Web Scraping and Data Processing Pipeline

This project is a data pipeline for crawling, processing, and storing data from various web sources. It uses [Prefect](https://www.prefect.io/) for workflow orchestration.

## Project Structure

The project is organized into the following directories:

-   `pipelines/`: Contains the main Prefect pipeline definition.
-   `web_scraping/`: Holds the web crawling and parsing logic for different data sources.
-   `database/`: Includes the database handler for uploading the processed data.
-   `datalake/`: Stores the raw data crawled from the web sources.
-   `logs/`: Contains logs generated by the pipeline.

## Getting Started

### Prerequisites

-   Python 3.12 or higher
-   Install the required dependencies from `requirements.txt`:
    ```bash
    pip install -r requirements.txt
    ```

### Running the Pipeline

The main pipeline is defined in `pipelines/load_source_pipeline.py` and can be run using Prefect. The pipeline requires a `source_slug` parameter to identify the data source to be processed.

To run the pipeline for a specific source, you can execute the following command:
``` bash
prefect deployment run 'load-source-data-pipeline/load-source-data-pipeline' --param source_slug=<your_source_slug>
```
Replace `<your_source_slug>` with the slug of the source you want to process.

## Pipeline Workflow

The `load_source_data_pipeline` consists of the following steps:

1.  **Crawl Data Source**:
    -   Dynamically imports the appropriate crawler class from the `web_scraping` directory based on the `source_slug`.
    -   Crawls the data from the web source.
    -   Saves the raw data to the `datalake`.

2.  **Process New Data**:
    -   Dynamically imports the corresponding parser class from the `web_scraping` directory.
    -   Parses the new data from the `datalake`.
    -   Saves the processed data.

3.  **Upload New Data**:
    -   Dynamically imports the relevant database handler from the `database` directory.
    -   Uploads the parsed data to the database
